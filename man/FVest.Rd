% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/FVest.R
\name{FVest}
\alias{FVest}
\title{Predict fitted values from Machine Learning model}
\usage{
FVest(
  model,
  X,
  Y,
  Xnew = X,
  Ynew = Y,
  ML = c("Lasso", "Ridge", "RF", "CIF", "XGB", "CB", "Logit_lasso", "OLS", "grf", "SL",
    "OLSensemble"),
  polynomial.Lasso = 1,
  polynomial.Ridge = 1,
  polynomial.Logit_lasso = 1,
  polynomial.OLS = 1,
  coefs = NULL
)
}
\arguments{
\item{model}{is an estimated Machine Learning model. Typically
a class S3 or S4 object.}

\item{X}{is a dataframe containing all the features on which the
model was estimated}

\item{Y}{is a vector containing the labels for which the model
was estimated}

\item{Xnew}{is a dataframe containing the features at which we
we want the predictions. Default is Xnew = X.}

\item{Ynew}{is a vector which needs to have length equal to the
rows of Xnew. It only matters that it has correct length so one
could use a vectors of zeros.}

\item{ML}{is a string specifying which machine learner to use}

\item{polynomial.Lasso}{degree of polynomial to be fitted when using Lasso.
1 just fits the input X. 2 squares all variables and adds
all pairwise interactions. 3 squares and cubes all variables and adds all
pairwise and threewise interactions...}

\item{polynomial.Ridge}{degree of polynomial to be fitted when using Ridge,
see polynomial.Lasso for more info.}

\item{polynomial.Logit_lasso}{degree of polynomial to be fitted when using Logit_lasso,
see polynomial.Lasso for more info.}

\item{polynomial.OLS}{degree of polynomial to be fitted when using OLS,
see polynomial.Lasso for more info.}

\item{coefs}{optimal coefficients for OLSensemble, computed in modest}
}
\value{
vector with fitted values
}
\description{
\code{FVest} takes an estimated machine learning model (Lasso, Ridge,
Random Forest, Conditional Inference Forest,
Extreme Gradient Boosting, Catboosting, Logit lasso or any
combination of these using the SuperLearner package) and returns
the predicted fitted values for Xnew.
}
\details{
Note that the glmnet package which implements Lasso and Ridge
does not handle factor variables (such as the ones in mad2019)
hence for this machine learners, modest turns X into model.matrix(~.,X)
which will perform dummy encoding on factor variables.
}
\examples{
X <- dplyr::select(mad2019,-Y)
Y <- mad2019$Y
m <- modest(X,Y,"RF")
FVest(m,X,Y,X[1:5,],Y[1:5],ML = "RF")

m <- modest(X,Y,"XGB")
FVest(m,X,Y,ML = "XGB")

m <- modest(X,Y,"SL",
ensemble = c("SL.Lasso","SL.RF","SL.XGB"))
FVest(m,X,Y,ML = "SL")


}
