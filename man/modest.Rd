% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modest.R
\name{modest}
\alias{modest}
\title{Estimate Machine Learning model}
\usage{
modest(
  X,
  Y,
  ML = c("Lasso", "Ridge", "RF", "CIF", "XGB", "CB", "Logit_lasso", "OLS", "grf", "SL",
    "OLSensemble"),
  OLSensemble,
  SL.library,
  rf.cf.ntree = 500,
  rf.depth = NULL,
  mtry = max(floor(ncol(X)/3), 1),
  polynomial = 1,
  ensemblefolds = 10,
  xgb.nrounds = 200,
  xgb.max.depth = 6,
  cb.iterations = 1000,
  cb.depth = 6,
  weights = NULL
)
}
\arguments{
\item{X}{is a dataframe containing all the features}

\item{Y}{is a vector containing the label}

\item{ML}{is a string specifying which machine learner to use}

\item{OLSensemble}{is a string vector specifying which learners
should be used in OLS ensemble method}

\item{SL.library}{is a string vector specifying which learners
should be used in SuperLearner}

\item{rf.cf.ntree}{how many trees should be grown when using RF or CIF}

\item{rf.depth}{how deep should trees be grown in RF (NULL is default from ranger)}

\item{polynomial}{degree of polynomial to be fitted when using Lasso, Ridge,
Logit Lasso or OLS. 1 just fits the input X. 2 squares all variables and adds
all pairwise interactions. 3 squares and cubes all variables and adds all
pairwise and threewise interactions...}

\item{ensemblefolds}{is an integer specifying how many folds to use in ensemble
methods such as OLSensemble or SuperLearner}

\item{xgb.nrounds}{is an integer specifying how many rounds to use in XGB}

\item{xgb.max.depth}{is an integer specifying how deep trees should be grown in XGB}

\item{cb.iterations}{The maximum number of trees that can be built in CB}

\item{cb.depth}{The depth of the trees in CB}

\item{weights}{is a vector containing survey weights adding up to 1}
}
\value{
the object that the machine learner package returns, in case of OLSensemble
it returns the coefficients assigned to each machine learner in ensemble
}
\description{
\code{modest} estimates the model for a specified machine learner,
possible options are Lasso, Ridge, Random Forest, Conditional
Inference Forest, Extreme Gradient Boosting, Catboosting, Logit lasso
or any combination of these using the SuperLearner package
}
\details{
Note that the glmnet package
which implements Lasso and Ridge does not handle factor variables
(such as the ones in mad2019), hence for this machine learners,
modest turns X into model.matrix(~.,X) which will perform dummy
encoding on factor variables.
}
\examples{
X <- dplyr::select(mad2019,-Y)
Y <- mad2019$Y
modest(X,Y,"RF")
modest(X,Y,"XGB")
modest(X,Y,"Lasso")
modest(X,Y,"SL",
ensemble = c("SL.Lasso","SL.Ridge","SL.RF","SL.CIF","SL.XGB","SL.CB"))

}
